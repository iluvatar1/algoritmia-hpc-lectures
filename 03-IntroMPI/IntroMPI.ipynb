{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb9a949",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to MPI\n",
    "- [MPI](https://www.mpi-forum.org/) stands for Message Passing Interface\n",
    "- [Distributed Memory](https://en.wikipedia.org/wiki/Distributed_memory?useskin=vector)\n",
    "- A library specification\n",
    "- Private memory space\n",
    "- Communication is needed: sender an receiver cooperate\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.iue.tuwien.ac.at/phd/weinbub/figures/distributedmemory.png\" alt=\"Image Description\" width=\"800\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2486f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Extensive documentation (very much needed, 6 basic functions, +125 functions in total!)\n",
    "- [MPI tutorial](https://mpitutorial.com/tutorials/) \n",
    "- [OpenMPI docs](https://www.open-mpi.org/doc/current/)\n",
    "- [MPI Cheat sheet](https://cheatography.com/test2000/cheat-sheets/parallel/)\n",
    "- [Archer2 training](https://www.archer2.ac.uk/training/courses/200514-mpi/)\n",
    "- [llnl Tutorial](https://hpc-tutorials.llnl.gov/mpi/)\n",
    "- [PRC](https://researchcomputing.princeton.edu/education/external-online-resources/mpi)\n",
    "- [Prace Pdc support](https://pdc-support.github.io/introduction-to-mpi/about/index.html)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560973c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MPI function calls/communication characteristics\n",
    "- Point to point\n",
    "- Collective\n",
    "- Blocking and non-blocking\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <div style=\"text-align: center;\">\n",
    "    <img src=\"https://hpc-tutorials.llnl.gov/mpi/images/buffer_recv.gif\" alt=\"Image Description\" width=\"600\">\n",
    "</div>\n",
    "      </td>\n",
    "    <td>\n",
    "        <div style=\"text-align: center;\">\n",
    "    <img src=\"https://hpc-tutorials.llnl.gov/mpi/images/collective_comm.gif\" alt=\"Image Description\" width=\"600\">\n",
    "</div>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td>\n",
    "          <div style=\"text-align: center;\">\n",
    "    <img src=\"http://www.md-esg.eu/wp-content/uploads/2021/10/juwels_lammps_scaling.png\" alt=\"Image Description\" width=\"600\">\n",
    "</div>\n",
    "      </td>\n",
    "      <td>\n",
    "         <div style=\"text-align: center;\">\n",
    "    <img src=\"https://hpc-tutorials.llnl.gov/mpi/images/Cartesian_topology.gif\" alt=\"Image Description\" width=\"600\">\n",
    "</div> \n",
    "      </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd438e8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MPI essential functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5efb425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T16:56:11.594439Z",
     "start_time": "2023-06-14T16:56:11.580103Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpi_basic.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpi_basic.cpp\n",
    "#include \"mpi.h\"\n",
    "#include <iostream>\n",
    "\n",
    "  int main(int argc, char **argv)\n",
    "  {\n",
    "    int processId;                 /* rank of process */\n",
    "    int noProcesses;               /* number of processes */\n",
    "\n",
    "    MPI_Init(&argc, &argv);                   /* Mandatory */\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &noProcesses);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &processId);\n",
    "      \n",
    "    std::cout << \"Hello from process \" << processId << \" of \" << noProcesses << \"\\n\";\n",
    "\n",
    "    MPI_Finalize();                       /* Mandatory */\n",
    "\n",
    "    return 0;\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b73ced",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To compile, use `mpic++` (to understand what it does, use `mpic++ --showme`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf4251e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T16:56:14.339923Z",
     "start_time": "2023-06-14T16:56:13.791839Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mpic++ mpi_basic.cpp -o mpi_basic.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a6adf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And no run it with `mpirun`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b26527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T16:56:56.790303Z",
     "start_time": "2023-06-14T16:56:55.361567Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from process 1 of 2\r\n",
      "Hello from process 0 of 2\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 2 ./mpi_basic.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae03e40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Functions explanations:\n",
    "- [`MPI_Init`](https://www.open-mpi.org/doc/current/man3/MPI_Init.3.php)\n",
    "- [`MPI_Comm_size`](https://www.open-mpi.org/doc/current/man3/MPI_Comm_size.3.php)\n",
    "- [`MPI_Comm_rank`](https://www.open-mpi.org/doc/current/man3/MPI_Comm_size.3.php)\n",
    "- [`MPI_Finnalize`](https://www.open-mpi.org/doc/current/man3/MPI_Finalize.3.php)\n",
    "\n",
    "Notice that you are running almost identical processes, the same source code, only the rank changes. By using that, you can differentiate among processes.\n",
    "\n",
    "The processes can run on the same machine, or on several machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df018f4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "Launch the same program using slurm, forst on the sme node, then on several nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19b2f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T17:01:56.474279Z",
     "start_time": "2023-06-14T17:01:56.472022Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Put the slurm script here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3b0b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MPI practical example: sum of large array\n",
    "This is analogous to the openmp example. But now the array can be as large as the memory for all computers is in total. First let's write the the serial version, then we will solve the problem using both point-to-point comms and later with collective comms. Finally, the same will be done in `python`. \n",
    "\n",
    "## Serial version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c34e3a4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:00:27.489300Z",
     "start_time": "2023-06-14T19:00:27.479036Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serial_main.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile serial_main.cpp\n",
    "#include \"serial_avg.h\"\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    const int N = std::stoi(argv[1]);\n",
    "    std::vector<double> data(N, 0.0);\n",
    "    \n",
    "    initialize(data);\n",
    "    \n",
    "    compute_avg(data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b4d714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:00:31.017649Z",
     "start_time": "2023-06-14T19:00:31.014221Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serial_avg.h\n"
     ]
    }
   ],
   "source": [
    "%%writefile serial_avg.h\n",
    "#pragma once\n",
    "#include <vector>\n",
    "#include <random>\n",
    "#include <iostream>\n",
    "void initialize(std::vector<double> & array);\n",
    "void compute_avg(const std::vector<double> & array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abbb2516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:00:33.658322Z",
     "start_time": "2023-06-14T19:00:33.652541Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serial_avg.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile serial_avg.cpp\n",
    "#include \"serial_avg.h\"\n",
    "void initialize(std::vector<double> & array)\n",
    "{\n",
    "    std::mt19937 gen(0); // 0 == seed\n",
    "    std::uniform_real_distribution<double> dis(0.0, 1.0);\n",
    "    for (auto & x : array) {\n",
    "        x = dis(gen);\n",
    "    }\n",
    "}\n",
    "\n",
    "void compute_avg(const std::vector<double> & array) {\n",
    "    double result = 0.0;\n",
    "    for (auto & x : array) {\n",
    "        result += x;\n",
    "    }\n",
    "    std::cout.setf(std::ios::scientific);\n",
    "    std::cout.precision(16);\n",
    "    std::cout << \"avg: \" << result/array.size();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23333caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:01:04.679603Z",
     "start_time": "2023-06-14T19:01:02.526898Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ...\n",
      "Executing ...\n",
      "avg: 5.0005594736679237e-01"
     ]
    }
   ],
   "source": [
    "!echo \"Compiling ...\"\n",
    "!g++ -std=c++17 -O3  serial_main.cpp serial_avg.cpp -o serial_array_avg.x\n",
    "!echo \"Executing ...\"\n",
    "!./serial_array_avg.x 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5aec1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Point to point parallel version\n",
    "In this case we will split the work among processes, and then use send and receive to communicate the partial sums and finally print the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49d1d14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T17:51:29.640261Z",
     "start_time": "2023-06-14T17:51:29.630694Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parallel_main.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile parallel_main.cpp\n",
    "#include \"parallel_avg_pointpoint.h\"\n",
    "#include \"mpi.h\"\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    // MPI initialization\n",
    "    MPI_Init(&argc, &argv); \n",
    "    int pid, np;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "    \n",
    "    // local problem definition\n",
    "    const int N = std::stoi(argv[1]); // total size\n",
    "    int Nlocal = N/np; // size for this process\n",
    "    std::vector<double> data(Nlocal, 0.0);\n",
    "    \n",
    "    initialize(data, pid, np);\n",
    "    \n",
    "    compute_avg(data, pid, np);\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "787fbb9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T17:51:40.678285Z",
     "start_time": "2023-06-14T17:51:40.667090Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parallel_avg.h\n"
     ]
    }
   ],
   "source": [
    "%%writefile parallel_avg.h\n",
    "#pragma once\n",
    "#include <vector>\n",
    "#include <random>\n",
    "#include <iostream>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "void initialize(std::vector<double> & array, int pid, int np);\n",
    "void compute_avg(const std::vector<double> & array, int pid, int np);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5275b312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T17:52:00.392153Z",
     "start_time": "2023-06-14T17:52:00.384439Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parallel_avg_pointpoint.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile parallel_avg_pointpoint.cpp\n",
    "#include \"parallel_avg.h\"\n",
    "void initialize(std::vector<double> & array, int pid, int np)\n",
    "{\n",
    "    std::mt19937 gen(pid); // pid == seed (what if seed == 0?)\n",
    "    std::uniform_real_distribution<double> dis(0.0, 1.0);\n",
    "    for (auto & x : array) {\n",
    "        x = dis(gen);\n",
    "    }\n",
    "}\n",
    "\n",
    "void compute_avg(const std::vector<double> & array, int pid, int np) {\n",
    "    double result = 0.0;\n",
    "    for (auto & x : array) {\n",
    "        result += x;\n",
    "    }\n",
    "    // mpi communication: TODO\n",
    "    if (0 == pid) {\n",
    "        // receive and accumulate\n",
    "    } else {\n",
    "        // send\n",
    "    }\n",
    "    // only master prints\n",
    "    std::cout.setf(std::ios::scientific);\n",
    "    std::cout.precision(16);\n",
    "    std::cout << \"avg: \" << result/array.size();\n",
    "    std::cout << pid << \"\\n\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4054a123",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T17:52:14.636864Z",
     "start_time": "2023-06-14T17:52:08.320692Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ...\n",
      "Executing ...\n",
      "avg: 4.9993301343222502e-011\n",
      "avg: 5.0003528264511499e-010\n"
     ]
    }
   ],
   "source": [
    "!echo \"Compiling ...\"\n",
    "!mpic++ -std=c++17 parallel_main.cpp parallel_avg_pointpoint.cpp -o parallel_array_pointpoint.x\n",
    "!echo \"Executing ...\"\n",
    "!mpirun -np 2 parallel_array_pointpoint.x 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2063d3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communications version\n",
    "In this case, a reduction operation would be handy and easier. Please read the documentation and implement the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7216140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T17:53:41.829405Z",
     "start_time": "2023-06-14T17:53:41.818864Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parallel_avg_collective.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile parallel_avg_collective.cpp\n",
    "#include \"parallel_avg.h\"\n",
    "void initialize(std::vector<double> & array, int pid, int np)\n",
    "{\n",
    "    std::mt19937 gen(pid); // pid == seed (what if seed == 0?)\n",
    "    std::uniform_real_distribution<double> dis(0.0, 1.0);\n",
    "    for (auto & x : array) {\n",
    "        x = dis(gen);\n",
    "    }\n",
    "}\n",
    "\n",
    "void compute_avg(const std::vector<double> & array, int pid, int np) {\n",
    "    double result = 0.0;\n",
    "    for (auto & x : array) {\n",
    "        result += x;\n",
    "    }\n",
    "    // TODO: create reduction, collective communication\n",
    "    \n",
    "    // TODO: only master prints \n",
    "    std::cout.setf(std::ios::scientific);\n",
    "    std::cout.precision(16);\n",
    "    std::cout << \"avg: \" << result/array.size();\n",
    "    std::cout << pid << \"\\n\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3fb4194",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T17:53:49.568683Z",
     "start_time": "2023-06-14T17:53:44.074609Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ...\n",
      "Executing ...\n",
      "avg: 4.9993301343222502e-011\n",
      "avg: 5.0003528264511499e-010\n"
     ]
    }
   ],
   "source": [
    "!echo \"Compiling ...\"\n",
    "!mpic++ -std=c++17 parallel_main.cpp parallel_avg_collective.cpp -o parallel_array_pointpoint.x\n",
    "!echo \"Executing ...\"\n",
    "!mpirun -np 2 parallel_array_pointpoint.x 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf0bee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parallel metrics for both point-to-point and collective versions\n",
    "Using `slurm`, compute the parallel metrics for both the point-to-point and the collective communication versions. Run both on a single computer and on several nodes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27025a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a4e68b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python mpi\n",
    "You can use the [`mpi4py`](https://mpi4py.readthedocs.io/en/stable/) package to call mpi from python. \n",
    "First, let's implement the point to point example. Take into account that `mpi4py` can serialize arbitrary objects using python pickle, but that is not optimal, so we better use `numpy` directly\n",
    "## Point to point comms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1e29127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:01:24.386710Z",
     "start_time": "2023-06-14T19:01:24.377084Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sum_pointpoint.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sum_pointpoint.py\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def initiliaze(size, pid, npr):\n",
    "    np.random.seed(pid)\n",
    "    return np.random.uniform(0.0, 1.0, size=size)\n",
    "    \n",
    "def parallel_avg(data, pid, npr, comm):\n",
    "    tag = 11\n",
    "    sumLocal = np.sum(data)\n",
    "    if pid >= 1:\n",
    "        comm.Send([sumLocal, MPI.DOUBLE], dest=0, tag=tag)\n",
    "    elif 0 == pid:\n",
    "        sumTotal = sumLocal\n",
    "        buf = np.empty(1, dtype=np.float64)\n",
    "        for ii in range(1, npr):\n",
    "            comm.Recv([buf, MPI.DOUBLE], source=ii, tag=tag)\n",
    "            sumTotal += buf[0]\n",
    "        N = data.size*npr\n",
    "        print(f\"avg: {sumTotal/N:20.15e}\")\n",
    "        \n",
    "def main():\n",
    "    # Initialize MPI\n",
    "    comm = MPI.COMM_WORLD\n",
    "    npr = comm.Get_size()\n",
    "    pid = comm.Get_rank()\n",
    "    N = int(sys.argv[1])\n",
    "    Nlocal = int(N/npr)\n",
    "    data = initiliaze(Nlocal, pid, npr)\n",
    "    parallel_avg(data, pid, npr, comm)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c87c8b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:02:23.677796Z",
     "start_time": "2023-06-14T19:02:22.649549Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 5.000068163660500e-01\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 10 python sum_pointpoint.py 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414e3cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now implement a `slurm` script and compute the parallel metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f9a9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "378593ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Collective communications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2f3aabc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:09:03.690587Z",
     "start_time": "2023-06-14T19:09:03.687331Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sum_collective.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sum_collective.py\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def initiliaze(size, pid, npr):\n",
    "    np.random.seed(pid)\n",
    "    return np.random.uniform(0.0, 1.0, size=size)\n",
    "    \n",
    "def parallel_avg(data, pid, npr, comm):\n",
    "    tag = 11\n",
    "    sumLocal = np.sum(data)\n",
    "    sumTotal = comm.allreduce(sumLocal, op=MPI.SUM) # What is allreduce?\n",
    "    #comm.Reduce([sumLocal, MPI.DOUBLE], [sumTotal, MPI.DOUBLE], op=MPI.SUM, root=0)\n",
    "    if 0 == pid:\n",
    "        N = data.size*npr\n",
    "        print(f\"avg: {sumTotal/N:20.15e}\")\n",
    "        \n",
    "def main():\n",
    "    # Initialize MPI\n",
    "    comm = MPI.COMM_WORLD\n",
    "    npr = comm.Get_size()\n",
    "    pid = comm.Get_rank()\n",
    "    N = int(sys.argv[1])\n",
    "    Nlocal = int(N/npr)\n",
    "    data = initiliaze(Nlocal, pid, npr)\n",
    "    parallel_avg(data, pid, npr, comm)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "681ebf95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T19:09:25.197755Z",
     "start_time": "2023-06-14T19:09:24.042543Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 5.000068163660500e-01\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 10 python sum_collective.py 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e582552e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now implement a `slurm` script and compute the parallel metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be8636",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a lot more to learn regarding MPI, like [blocking/unblocking](https://hpc-tutorials.llnl.gov/mpi/routine_args/) ops, many other types of [collective comms](https://www.mpi-forum.org/docs/mpi-1.1/mpi-11-html/node64.html), [topologies](https://hpc-tutorials.llnl.gov/mpi/virtual_topologies/), [parallel IO](https://docs.alliancecan.ca/wiki/Parallel_I/O_introductory_tutorial) (but better do it using hdf5 or netcdf), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cdd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
