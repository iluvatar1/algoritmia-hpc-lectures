{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9c3d64",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming\n",
    "\n",
    "[GPU Programming](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units?useskin=vector) allows to program masivelly parallel programs on graphics devices, getting up to 1000x speedups, using protocols like [CUDA](https://en.wikipedia.org/wiki/CUDA?useskin=vector) ([nvidia site](https://developer.nvidia.com/cuda-zone)), [OpenCL](https://en.wikipedia.org/w/index.php?title=OpenCL&useskin=vector), [OpenACC](https://en.wikipedia.org/wiki/OpenACC?useskin=vector), etc.\n",
    "\n",
    "Brief history:\n",
    "- 1980s: The first GPUs were developed for use in scientific computing and computer graphics.\n",
    "- 1990s: GPUs began to be used in video games, and the first programming languages for GPUs were developed.\n",
    "- 2000s: GPUs became more powerful and affordable, and their use in scientific computing and machine learning grew.\n",
    "- 2010s: GPUs became the dominant platform for machine learning and artificial intelligence, and new programming languages and frameworks were developed to make GPU programming easier.\n",
    "- 2020s: GPUs continue to evolve, with new architectures and features being introduced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3991da7",
   "metadata": {},
   "source": [
    "Here are some specific examples of GPU programming languages and frameworks:\n",
    "\n",
    "- CUDA: A parallel computing platform developed by Nvidia.\n",
    "- OpenCL: An open standard for parallel programming of heterogeneous systems.\n",
    "- TensorFlow: A free and open-source software library for machine learning.\n",
    "- PyTorch: A Python library for machine learning.\n",
    "\n",
    "Here are some additional resources that you may find helpful:\n",
    "\n",
    "- GPU Programming Tutorial: https://www.learnopengl.com/\n",
    "- CUDA Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\n",
    "- OpenCL Programming Guide: https://www.khronos.org/registry/OpenCL/specs/opencl-1.2.pdf\n",
    "- TensorFlow Tutorial: https://www.tensorflow.org/tutorials/quickstart/beginner\n",
    "- PyTorch Tutorial: https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "- https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/\n",
    "- https://developer.nvidia.com/blog/even-easier-introduction-cuda/\n",
    "- https://hpc-wiki.info/hpc/GPU_Tutorial/SAXPY_CUDA_C\n",
    "- https://surma.dev/things/webgpu/\n",
    "- https://carpentries-incubator.github.io/lesson-gpu-programming/\n",
    "- https://enccs.github.io/gpu-programming/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e12d38",
   "metadata": {},
   "source": [
    "## Components trend\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://enccs.github.io/gpu-programming/_images/microprocessor-trend-data.png\" alt=\"Image Description\" width=\"800\">\n",
    "    <figcaption>https://enccs.github.io/gpu-programming/_images/microprocessor-trend-data.png</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd90bc1",
   "metadata": {},
   "source": [
    "## CPU - GPU\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://enccs.github.io/gpu-programming/_images/CPUAndGPU.png\" alt=\"Image Description\" width=\"800\">\n",
    "    <figcaption>From: \"https://enccs.github.io/gpu-programming/_images/CPUAndGPU.png\"</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873345a",
   "metadata": {},
   "source": [
    "## Thread hierarchy\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/06/kernel-execution-on-gpu-1.png\" alt=\"Image Description\" width=\"600\">\n",
    "    <figcaption>From: \"https://developer-blogs.nvidia.com/wp-content/uploads/2020/06/kernel-execution-on-gpu-1.png\"</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e999473",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "## Local (sala2)\n",
    "Here we will setup a computer which has an `Nvidia Quadro P1000` card. You need to install both the driver and the cuda toolkit (the later better to be installed as a part of the nvidia sdk)\n",
    "\n",
    "- Driver download for quadro P1000: https://www.nvidia.com/Download/driverResults.aspx/204639/en-us/\n",
    "- Nvidia sdk: https://developer.nvidia.com/hpc-sdk-downloads\n",
    "- Nvidia singularity: This is the recommended way. The image is built at /packages/nvhpc23.3devel.sif. More instructions at https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc \n",
    "\n",
    "    - Accesing a shell inside the container but with visibility to all user account files:\n",
    "    ```bash\n",
    "           singularity shell --nv /packages/nvhpc_23.3_devel.sif\n",
    "    ```\n",
    "    - Compiling\n",
    "      ```bash\n",
    "      singularity exec --nv /packages/nvhpc_23.3_devel.sif nvc++ -g cuda_02.cu\n",
    "      ```\n",
    "    - Executing with nvprof\n",
    "      ```bash\n",
    "      singularity exec --nv /packages/nvhpc_23.3_devel.sif nvprof ./a.out\n",
    "      ```\n",
    "- Local module: Load the nvidia sdk (sala2):\n",
    "  ```bash  \n",
    "    module load /packages/nvidia/hpc_sdk/modulefiles/nvhpc/23.3\n",
    "  ```\n",
    "  Compile as\n",
    "  ```bash\n",
    "   nvc++  -std=c++17 -o offload.x offload.cpp\n",
    "  ```\n",
    "- Docker: The docker container is installed. Unfortunately it does not run since the device compute capability is not enough\n",
    "   ```bash\n",
    "   docker run --gpus all -it --rm nvcr.io/nvidia/nvhpc:23.3-devel-cuda_multi-ubuntu20.04\n",
    "          docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\n",
    "   ```\n",
    "\n",
    "More info about container: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc1fa6",
   "metadata": {},
   "source": [
    "## Google collab\n",
    "Open a collab notebook, go to runtime, change runtime type, hardware accelerator -> GPU, GPU type -> T4, Save. The you will have a runtime with a T4 card, for free. If you want an even better card, you can pay for collab pro.\n",
    "\n",
    "Inside the notebook, you can run commands with the prefix ! to run then as in a console. For instance, to get the device properties, you can run \n",
    "```python\n",
    "!nvidia-smi\n",
    "```\n",
    "and you should get something like\n",
    "```txt\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   44C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576bc4a",
   "metadata": {},
   "source": [
    "To create local files, like filename.cu, use the magic `%%writefile filename.cu` at the beginning of the cell and then put the file contents in the same cell. \n",
    "\n",
    " Finally, to compile and run just execute the following\n",
    "```python\n",
    "!nvcc filename.cu -o name.x\n",
    "!nvprof ./name.x\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9d267",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "We will follow https://developer.nvidia.com/blog/even-easier-introduction-cuda/\n",
    "\n",
    "## C++ basic example: Adding two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6166401a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T21:43:07.951644Z",
     "start_time": "2023-06-23T21:43:07.938107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cuda_01.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile cuda_01.cpp\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "// function to add the elements of two arrays\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20; // 1M elements\n",
    "\n",
    "  float *x = new float[N];\n",
    "  float *y = new float[N];\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the CPU\n",
    "  add(N, x, y);\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  delete [] x;\n",
    "  delete [] y;\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ab6166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T21:50:53.345460Z",
     "start_time": "2023-06-23T21:50:52.375411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0\r\n"
     ]
    }
   ],
   "source": [
    "!g++ -g -std=c++17 cuda_01.cpp\n",
    "!./a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb76c47",
   "metadata": {},
   "source": [
    "## Cuda  example\n",
    "(See https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/ or https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/)\n",
    "\n",
    "Here we add an annotation to allow our function to run on the device. Also, now we need to transfer memory to and from the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211db238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T22:05:26.110866Z",
     "start_time": "2023-06-23T22:05:26.104809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cuda_02.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile cuda_02.cu\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "// Kernel function to add the elements of two arrays\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20;\n",
    "  float *x, *y;\n",
    "\n",
    "  // Allocate Unified Memory – accessible from CPU or GPU\n",
    "  cudaMallocManaged(&x, N*sizeof(float));\n",
    "  cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the GPU\n",
    "  add<<<1, 1>>>(N, x, y);\n",
    "\n",
    "  // Wait for GPU to finish before accessing on host\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48046b2f",
   "metadata": {},
   "source": [
    "### Local compilation\n",
    "To compile, run\n",
    "```bash\n",
    "singularity exec --nv /packages/nvhpc_23.3_devel.sif nvc++ -g cuda_02.cu\n",
    "singularity exec --nv /packages/nvhpc_23.3_devel.sif ./a.out\n",
    "singularity exec --nv /packages/nvhpc_23.3_devel.sif nvprof ./a.out\n",
    "```\n",
    "then you will get something like\n",
    "```bash\n",
    "==16094== NVPROF is profiling process 16094, command: ./a.out\n",
    "Max error: 0\n",
    "==16094== Profiling application: ./a.out\n",
    "==16094== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:  100.00%  2.54774s         1  2.54774s  2.54774s  2.54774s  add(int, float*, float*)\n",
    "      API calls:   93.27%  2.54776s         1  2.54776s  2.54776s  2.54776s  cudaDeviceSynchronize\n",
    "                    6.71%  183.20ms         2  91.602ms  20.540us  183.18ms  cudaMallocManaged\n",
    "                    0.02%  468.25us         2  234.13us  216.27us  251.98us  cudaFree\n",
    "                    0.01%  213.75us       101  2.1160us     141ns  150.11us  cuDeviceGetAttribute\n",
    "                    0.00%  32.127us         1  32.127us  32.127us  32.127us  cudaLaunchKernel\n",
    "                    0.00%  22.239us         1  22.239us  22.239us  22.239us  cuDeviceGetName\n",
    "                    0.00%  6.1330us         1  6.1330us  6.1330us  6.1330us  cuDeviceGetPCIBusId\n",
    "                    0.00%  1.5730us         3     524ns     197ns  1.1650us  cuDeviceGetCount\n",
    "                    0.00%     808ns         2     404ns     141ns     667ns  cuDeviceGet\n",
    "                    0.00%     530ns         1     530ns     530ns     530ns  cuDeviceTotalMem\n",
    "                    0.00%     243ns         1     243ns     243ns     243ns  cuDeviceGetUuid\n",
    "\n",
    "==16094== Unified Memory profiling result:\n",
    "Device \"Quadro P1000 (0)\"\n",
    "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  735.2380us  Host To Device\n",
    "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  337.3770us  Device To Host\n",
    "      24         -         -         -           -  2.855987ms  Gpu page fault groups\n",
    "Total CPU Page faults: 36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148b768",
   "metadata": {},
   "source": [
    "### Collab compilation\n",
    "Compile as \n",
    "```bash \n",
    "!nvcc cuda_02.cu -o cuda_02.x\n",
    "```\n",
    "and then run as \n",
    "```bash\n",
    "!nvprof ./cuda_02.x\n",
    "```\n",
    "to get something like\n",
    "```bash\n",
    "==18853== NVPROF is profiling process 18853, command: ./cuda_02.x\n",
    "Max error: 0\n",
    "==18853== Profiling application: ./cuda_02.x\n",
    "==18853== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:  100.00%  108.83ms         1  108.83ms  108.83ms  108.83ms  add(int, float*, float*)\n",
    "      API calls:   72.48%  290.34ms         2  145.17ms  36.191us  290.31ms  cudaMallocManaged\n",
    "                   27.17%  108.84ms         1  108.84ms  108.84ms  108.84ms  cudaDeviceSynchronize\n",
    "                    0.28%  1.1298ms         2  564.90us  537.96us  591.84us  cudaFree\n",
    "                    0.05%  182.13us       101  1.8030us     264ns  75.268us  cuDeviceGetAttribute\n",
    "                    0.01%  48.553us         1  48.553us  48.553us  48.553us  cudaLaunchKernel\n",
    "                    0.01%  28.488us         1  28.488us  28.488us  28.488us  cuDeviceGetName\n",
    "                    0.00%  8.6520us         1  8.6520us  8.6520us  8.6520us  cuDeviceGetPCIBusId\n",
    "                    0.00%  2.3140us         3     771ns     328ns  1.6230us  cuDeviceGetCount\n",
    "                    0.00%     919ns         2     459ns     315ns     604ns  cuDeviceGet\n",
    "                    0.00%     580ns         1     580ns     580ns     580ns  cuDeviceTotalMem\n",
    "                    0.00%     532ns         1     532ns     532ns     532ns  cuModuleGetLoadingMode\n",
    "                    0.00%     382ns         1     382ns     382ns     382ns  cuDeviceGetUuid\n",
    "\n",
    "==18853== Unified Memory profiling result:\n",
    "Device \"Tesla T4 (0)\"\n",
    "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  809.9640us  Host To Device\n",
    "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.6320us  Device To Host\n",
    "      12         -         -         -           -  2.564287ms  Gpu page fault groups\n",
    "Total CPU Page faults: 36\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af2a39",
   "metadata": {},
   "source": [
    "**This is doing the same work per thread, and actually has a race condition since all threads are accessing the global array.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0683128",
   "metadata": {},
   "source": [
    "## Using better the device\n",
    "Here we will modify the execution configuration, `<<<...>>>`, to use better the intrinsic parallelism. The second parameter is the number of threads per block, which are usually multiples of 32. Once you change it, it is necessary to rewrite the kernel to split the work among threads, using the [special vars](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables) `threadIdx.x` (thread id in the block), and `blockDim.x` (number of threads in the block, in this case, 256).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial02/01_parallel_thread.png\" alt=\"Image Description\" width=\"900\">\n",
    "    <figcaption>From: \"https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial02/01_parallel_thread.png\"</figcaption>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3663e9",
   "metadata": {},
   "source": [
    "If you increase just the number of threads to 256 (check the change in <<<...>>>), and split correctly the work using the cuda vars `threadIdx.x`  and `blockDim.x` , as shown, \n",
    "```c++\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = threadIdx.x;\n",
    "  int stride = blockDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "```\n",
    "and run the kernel as\n",
    "```c++\n",
    "// Run kernel on 1M elements on the GPU\n",
    "  add<<<1, 256>>>(N, x, y);\n",
    "```\n",
    "Apply this changes, compile and run again. Compare with the following results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474dc6c",
   "metadata": {},
   "source": [
    "### Local : Quadro P1000\n",
    "From 2.5 secs to 0.022 secs! \n",
    "\n",
    "```c++\n",
    "==21739== NVPROF is profiling process 21739, command: ./a.out\n",
    "Max error: 0\n",
    "==21739== Profiling application: ./a.out\n",
    "==21739== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:  100.00%  21.978ms         1  21.978ms  21.978ms  21.978ms  add(int, float*, float*)\n",
    "      API calls:   87.86%  164.24ms         2  82.118ms  12.398us  164.22ms  cudaMallocManaged\n",
    "                   11.76%  21.980ms         1  21.980ms  21.980ms  21.980ms  cudaDeviceSynchronize\n",
    "                    0.24%  457.32us         2  228.66us  177.89us  279.43us  cudaFree\n",
    "                    0.11%  206.80us       101  2.0470us     128ns  144.81us  cuDeviceGetAttribute\n",
    "                    0.02%  29.041us         1  29.041us  29.041us  29.041us  cudaLaunchKernel\n",
    "                    0.01%  20.149us         1  20.149us  20.149us  20.149us  cuDeviceGetName\n",
    "                    0.00%  5.5860us         1  5.5860us  5.5860us  5.5860us  cuDeviceGetPCIBusId\n",
    "                    0.00%  2.1000us         3     700ns     277ns     958ns  cuDeviceGetCount\n",
    "                    0.00%     952ns         2     476ns     330ns     622ns  cuDeviceGet\n",
    "                    0.00%     391ns         1     391ns     391ns     391ns  cuDeviceTotalMem\n",
    "                    0.00%     259ns         1     259ns     259ns     259ns  cuDeviceGetUuid\n",
    "\n",
    "==21739== Unified Memory profiling result:\n",
    "Device \"Quadro P1000 (0)\"\n",
    "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  734.5940us  Host To Device\n",
    "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  338.5950us  Device To Host\n",
    "      24         -         -         -           -  1.764587ms  Gpu page fault groups\n",
    "Total CPU Page faults: 36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92246749",
   "metadata": {},
   "source": [
    "### Google collab: Tesla T4\n",
    "From 0.108 secs to 0.004 secs! \n",
    "```c++\n",
    "==21448== NVPROF is profiling process 21448, command: ./cuda_03.x\n",
    "Max error: 0\n",
    "==21448== Profiling application: ./cuda_03.x\n",
    "==21448== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:  100.00%  3.7978ms         1  3.7978ms  3.7978ms  3.7978ms  add(int, float*, float*)\n",
    "      API calls:   98.24%  291.22ms         2  145.61ms  73.005us  291.15ms  cudaMallocManaged\n",
    "                    1.28%  3.8044ms         1  3.8044ms  3.8044ms  3.8044ms  cudaDeviceSynchronize\n",
    "                    0.36%  1.0699ms         2  534.95us  512.29us  557.62us  cudaFree\n",
    "                    0.08%  222.64us       101  2.2040us     174ns  102.62us  cuDeviceGetAttribute\n",
    "                    0.02%  62.588us         1  62.588us  62.588us  62.588us  cudaLaunchKernel\n",
    "                    0.02%  44.725us         1  44.725us  44.725us  44.725us  cuDeviceGetName\n",
    "                    0.00%  8.1290us         1  8.1290us  8.1290us  8.1290us  cuDeviceGetPCIBusId\n",
    "                    0.00%  3.2970us         3  1.0990us     266ns  2.6840us  cuDeviceGetCount\n",
    "                    0.00%  1.7320us         2     866ns     352ns  1.3800us  cuDeviceGet\n",
    "                    0.00%     632ns         1     632ns     632ns     632ns  cuDeviceTotalMem\n",
    "                    0.00%     549ns         1     549ns     549ns     549ns  cuModuleGetLoadingMode\n",
    "                    0.00%     377ns         1     377ns     377ns     377ns  cuDeviceGetUuid\n",
    "\n",
    "==21448== Unified Memory profiling result:\n",
    "Device \"Tesla T4 (0)\"\n",
    "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  825.8720us  Host To Device\n",
    "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.3130us  Device To Host\n",
    "      13         -         -         -           -  2.951606ms  Gpu page fault groups\n",
    "Total CPU Page faults: 36\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2073f",
   "metadata": {},
   "source": [
    "## Multiprocessors\n",
    " Cuda devices group parallel processors into Streaming Multiprocessors (SM), and each of them can run several thread blocks in parallel. In our case, by using the command deviceQuery (for the QuadroP1000 system it is at /opt/cuda/extras/demo_suite/deviceQuery), we get\n",
    "\n",
    "- Quadro P1000: 5 SM, 128 threads/SM\n",
    "- Tesla T4: 32 SM, 128 threads/SM\n",
    "\n",
    "So the ideal number of threads changes per card, and we will compute as \n",
    "```c++\n",
    "int blockSize = 128;\n",
    "int numBlocks = (N + blockSize - 1) / blockSize; // what if N is not divisible by blocksize?\n",
    "add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "```?\n",
    "\n",
    "We can control the number of thread blocks with the first parameter from the execution configuration `<<<>>>`. A set of parallel blocks of threads is known as the *grid*. \n",
    "\n",
    " The kernel will now become\n",
    "```c++\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "```\n",
    "based on the job distribution done by the tutorial \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\" alt=\"Image Description\" width=\"800\">\n",
    "    <figcaption>From: \"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\"</figcaption>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://enccs.github.io/gpu-programming/_images/MappingBlocksToSMs.png\" alt=\"Image Description\" width=\"800\">\n",
    "    <figcaption>From: \"https://enccs.github.io/gpu-programming/_images/MappingBlocksToSMs.png\"</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895400b",
   "metadata": {},
   "source": [
    "### Local: Nvidia Quadro P1000: From 2.500 to 0.022 to 0.006 secs! \n",
    "```c++\n",
    "==10662== NVPROF is profiling process 10662, command: ./a.out\n",
    "Max error: 0\n",
    "==10662== Profiling application: ./a.out\n",
    "==10662== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:  100.00%  6.0868ms         1  6.0868ms  6.0868ms  6.0868ms  add(int, float*, float*)\n",
    "      API calls:   96.03%  165.28ms         2  82.641ms  13.911us  165.27ms  cudaMallocManaged\n",
    "                    3.54%  6.0887ms         1  6.0887ms  6.0887ms  6.0887ms  cudaDeviceSynchronize\n",
    "                    0.27%  460.56us         2  230.28us  184.71us  275.85us  cudaFree\n",
    "                    0.13%  215.37us       101  2.1320us     133ns  151.55us  cuDeviceGetAttribute\n",
    "                    0.02%  30.822us         1  30.822us  30.822us  30.822us  cudaLaunchKernel\n",
    "                    0.01%  22.122us         1  22.122us  22.122us  22.122us  cuDeviceGetName\n",
    "                    0.00%  5.7430us         1  5.7430us  5.7430us  5.7430us  cuDeviceGetPCIBusId\n",
    "                    0.00%  1.3810us         3     460ns     203ns     945ns  cuDeviceGetCount\n",
    "                    0.00%     921ns         2     460ns     163ns     758ns  cuDeviceGet\n",
    "                    0.00%     438ns         1     438ns     438ns     438ns  cuDeviceTotalMem\n",
    "                    0.00%     234ns         1     234ns     234ns     234ns  cuDeviceGetUuid\n",
    "\n",
    "==10662== Unified Memory profiling result:\n",
    "Device \"Quadro P1000 (0)\"\n",
    "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "      59  138.85KB  4.0000KB  0.9961MB  8.000000MB  740.3880us  Host To Device\n",
    "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  337.8280us  Device To Host\n",
    "      32         -         -         -           -  2.253582ms  Gpu page fault groups\n",
    "Total CPU Page faults: 36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b621c",
   "metadata": {},
   "source": [
    "### Google collab: Testla T4: From 0.108 to 0.004 to 0.003 secs \n",
    "```c++ \n",
    "==8972== NVPROF is profiling process 8972, command: ./cuda_04.x\n",
    "Max error: 0\n",
    "==8972== Profiling application: ./cuda_04.x\n",
    "==8972== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:  100.00%  2.9741ms         1  2.9741ms  2.9741ms  2.9741ms  add(int, float*, float*)\n",
    "      API calls:   98.47%  250.63ms         2  125.31ms  38.785us  250.59ms  cudaMallocManaged\n",
    "                    1.18%  2.9959ms         1  2.9959ms  2.9959ms  2.9959ms  cudaDeviceSynchronize\n",
    "                    0.24%  613.16us         2  306.58us  302.27us  310.89us  cudaFree\n",
    "                    0.07%  188.26us       101  1.8630us     169ns  86.068us  cuDeviceGetAttribute\n",
    "                    0.02%  38.874us         1  38.874us  38.874us  38.874us  cuDeviceGetName\n",
    "                    0.01%  37.051us         1  37.051us  37.051us  37.051us  cudaLaunchKernel\n",
    "                    0.00%  5.7050us         1  5.7050us  5.7050us  5.7050us  cuDeviceGetPCIBusId\n",
    "                    0.00%  2.2980us         3     766ns     224ns  1.8050us  cuDeviceGetCount\n",
    "                    0.00%     979ns         2     489ns     195ns     784ns  cuDeviceGet\n",
    "                    0.00%     587ns         1     587ns     587ns     587ns  cuDeviceTotalMem\n",
    "                    0.00%     367ns         1     367ns     367ns     367ns  cuModuleGetLoadingMode\n",
    "                    0.00%     324ns         1     324ns     324ns     324ns  cuDeviceGetUuid\n",
    "\n",
    "==8972== Unified Memory profiling result:\n",
    "Device \"Tesla T4 (0)\"\n",
    "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "     106  77.282KB  4.0000KB  980.00KB  8.000000MB  969.6510us  Host To Device\n",
    "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  363.6760us  Device To Host\n",
    "      11         -         -         -           -  2.908132ms  Gpu page fault groups\n",
    "Total CPU Page faults: 36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326c662",
   "metadata": {},
   "source": [
    "# More to check\n",
    "- Openmp offload to gpu: https://www.youtube.com/watch?v=uVcvecgdW7g\n",
    "- OpenAcc:\n",
    "  - https://www.openacc.org/\n",
    "  - https://enccs.github.io/OpenACC-CUDA-beginners/1.02_openacc-introduction/\n",
    "  - https://ulhpc-tutorials.readthedocs.io/en/latest/gpu/openacc/basics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88c449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
